@InProceedings{ouyang2022training,
  author    = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Training language models to follow instructions with human feedback},
  year      = {2022},
}

@Article{Jordan2015,
  author    = {Jordan, Michael I and Mitchell, Tom M},
  journal   = {Science},
  title     = {Machine learning: Trends, perspectives, and prospects},
  year      = {2015},
}

@Article{Donoho2017,
  author    = {Donoho, David},
  journal   = {Journal of Computational and Graphical Statistics},
  title     = {50 years of data science},
  year      = {2017},
}

@Article{kaplan2020scaling,
  author  = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal = {arXiv preprint},
  title   = {Scaling laws for neural language models},
  year    = {2020},
}

@InProceedings{villalobosposition,
  author    = {Villalobos, Pablo and Ho, Anson and Sevilla, Jaime and Besiroglu, Tamay and Heim, Lennart and Hobbhahn, Marius},
  booktitle = {International Conference on Machine Learning},
  title     = {Position: Will we run out of data? {L}imits of {LLM} scaling based on human-generated data},
  year      = {2024},
}

@article{wolters2024memory,
  title={Memory is all you need: An overview of compute-in-memory architectures for accelerating large language model inference},
  author={Wolters, Christopher and Yang, Xiaoxuan and Schlichtmann, Ulf and Suzumura, Toyotaro},
  journal={arXiv preprint},
  year={2024}
}

@Article{duan2024llms,
  author  = {Duan, Hanyu and Yang, Yi and Tam, Kar Yan},
  journal = {arXiv preprint},
  title   = {Do {LLMs} know about Hallucination? An empirical investigation of {LLM's} hidden states},
  year    = {2024},
}